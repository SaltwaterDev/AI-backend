{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, \n",
    "                dataset_path, \n",
    "                dataset_cache=None\n",
    "               ):\n",
    "    \"\"\" Get tokenized dataset from directory or cache.\"\"\"\n",
    "\n",
    "    if dataset_path:\n",
    "        print(\"Download dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            \n",
    "        print(\"Tokenize and encode the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        dataset = tokenize(dataset)\n",
    "        torch.save(dataset, dataset_cache)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def get_data_loaders(dataset_path, dataset_cache=None, tokenizer=None):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = get_dataset(tokenizer, dataset_path, dataset_cache)\n",
    "\n",
    "    print(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        \"\"\"num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if args.num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(args.num_candidates, num_candidates)\"\"\"\n",
    "        # we only need 1 candidate -> the best comment\n",
    "        num_candidates = 1\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            personality_permutations = 1\n",
    "            for _ in range(personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    max_history = 1\n",
    "                    history = utterance[\"history\"][-(2*max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                #persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "\n",
    "    print(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    print(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    #train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(not args.distributed))\n",
    "    #valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\n",
    "\n",
    "    #print(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
    "    #print(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\n",
    "    \"\"\"let Trainer do the data loading\"\"\"\n",
    "    #return train_loader, valid_loader\n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
    "\n",
    "        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        # Freeze bert layers and only train the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classification layer\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "\n",
    "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n",
    "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n",
    "        # objective during pre-training.\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n",
    "freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
    "#maxlen = 128  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
    "bs = 16  # batch size\n",
    "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
    "lr = 2e-5  # learning rate\n",
    "epochs = 4  # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = counsel-chat/data/counsel_chat_250-tokens_full.json\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "print(\"Prepare datasets\")\n",
    "#train_loader, val_loader = get_data_loaders(dataset_path, tokenizer=tokenizer)\n",
    "train_dataset, val_dataset = get_data_loaders(dataset_path, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
    "\n",
    "        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        # Freeze bert layers and only train the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classification layer\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "\n",
    "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n",
    "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n",
    "        # objective during pre-training.\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_abThXlSr6n"
   },
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=epochs,              # total # of training epochs\n",
    "    per_device_train_batch_size=bs,  # batch size per device during training\n",
    "    per_device_eval_batch_size=bs,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset\n",
    "    learning_rate=lr,                    # learning rate\n",
    "    gradient_accumulation_steps=iters_to_accumulate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
